# -*- coding: utf-8 -*-
"""GenreClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1li90p7rjgNyJK7w1nHMJ-s0ihcQWEX6u
"""

import json
import re
import os
import pickle
import numpy as np
import random
from collections import Counter

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

import matplotlib.pyplot as plt

# ----------------------------------------
# CONFIG
# ----------------------------------------
JSON_PATH = "JazzStandards.json"
MODEL_DIR = "saved_model"
os.makedirs(MODEL_DIR, exist_ok=True)

MAX_SEQUENCE_LENGTH = 350
EMBEDDING_DIM = 96
LSTM_UNITS = 128
BATCH_SIZE = 32
EPOCHS = 45
RANDOM_STATE = 42

MIN_SAMPLES_PER_CLASS = 6   # drop categories ≤ 5 songs

# ----------------------------------------
# Helper: chord parsing
# ----------------------------------------

def parse_chords_from_segment(chords_string):
    if chords_string is None:
        return []
    bars = re.split(r'\|', chords_string)
    tokens = []
    for bar in bars:
        bar = bar.strip()
        if not bar:
            continue
        parts = re.split(r',', bar)
        for p in parts:
            p = p.strip().strip("() ")
            if p:
                tokens.append(p)
    return tokens


def tune_to_chord_sequence(tune):
    """Flatten chords across all sections and endings (preserving order)."""
    seq = []
    for sec in tune.get("Sections", []):
        main = sec.get("MainSegment", {})
        seq.extend(parse_chords_from_segment(main.get("Chords", "")))

        for ending in sec.get("Endings", []) or []:
            seq.extend(parse_chords_from_segment(ending.get("Chords", "")))

    return seq

# ----------------------------------------
# Transposition augmentation
# ----------------------------------------

NOTE_TO_INT = {
    "C":0, "C#":1, "Db":1,
    "D":2, "D#":3, "Eb":3,
    "E":4, "F":5, "E#":5,
    "F#":6, "Gb":6,
    "G":7, "G#":8, "Ab":8,
    "A":9, "A#":10,"Bb":10,
    "B":11,"Cb":11
}
INT_TO_NOTE = ["C","C#","D","D#","E","F","F#","G","G#","A","A#","B"]


def transpose_chord(chord, semitones):
    """Transpose root note of chord by semitones while keeping quality/extensions."""
    match = re.match(r"([A-G][b#]?)(.*)", chord)
    if not match:
        return chord

    root, rest = match.groups()
    if root not in NOTE_TO_INT:
        return chord

    new_int = (NOTE_TO_INT[root] + semitones) % 12
    new_root = INT_TO_NOTE[new_int]
    return new_root + rest


def transpose_sequence(seq, semitones):
    return [transpose_chord(c, semitones) for c in seq]

# ----------------------------------------
# Load dataset
# ----------------------------------------

def load_dataset(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ----------------------------------------
# Prepare data
# ----------------------------------------

def filter_rare_classes(data, min_count=6):
    """Drops rhythm categories with <= min_count samples."""
    counts = Counter([t["Rhythm"] for t in data])
    keep = {r for r, c in counts.items() if c >= min_count}
    print("[INFO] Keeping classes:", keep)
    print("[INFO] Dropping:", {r: c for r, c in counts.items() if r not in keep})
    return [t for t in data if t["Rhythm"] in keep]


def prepare_data(data, augment=True):
    chord_sequences = []
    rhythms = []

    for tune in data:
        seq = tune_to_chord_sequence(tune)
        if not seq:
            continue

        # Add original
        chord_sequences.append(seq)
        rhythms.append(tune["Rhythm"])

        # Add transposed versions (semi-tones: +1…+11)
        if augment:
            for semitones in range(1, 12):
                chord_sequences.append(transpose_sequence(seq, semitones))
                rhythms.append(tune["Rhythm"])

    # Convert sequences to strings for tokenizer
    sequence_strings = [" ".join(seq) for seq in chord_sequences]

    # Tokenizer
    tokenizer = Tokenizer(filters="", lower=False, oov_token="<OOV>")
    tokenizer.fit_on_texts(sequence_strings)
    X = tokenizer.texts_to_sequences(sequence_strings)
    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH, padding="post", truncating="post")

    # Labels
    le = LabelEncoder()
    y = le.fit_transform(rhythms)

    print(f"[INFO] Augmented dataset size: {len(X)} samples")
    print(f"[INFO] Vocab size: {len(tokenizer.word_index)+1}")
    print(f"[INFO] Classes: {list(le.classes_)}")

    return X, y, tokenizer, le

# ----------------------------------------
# Build model, using LSTM to handle sequential data
# ----------------------------------------

def build_model(vocab_size, num_classes):
    m = Sequential()
    m.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
    m.add(Bidirectional(LSTM(LSTM_UNITS, return_sequences=False)))
    m.add(Dropout(0.4))
    m.add(Dense(96, activation="relu"))
    m.add(Dropout(0.3))
    m.add(Dense(num_classes, activation="softmax"))
    return m

# ----------------------------------------
# Training pipeline
# ----------------------------------------

def train_pipeline():
    data = load_dataset(JSON_PATH)

    # Drop rare categories
    data = filter_rare_classes(data, MIN_SAMPLES_PER_CLASS)

    X, y, tokenizer, label_encoder = prepare_data(data, augment=True)

    # Train/val/test split
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_STATE
    )

    class_weights = compute_class_weight("balanced", classes=np.unique(y_train), y=y_train)
    class_weights = {i: w for i, w in enumerate(class_weights)}

    vocab_size = len(tokenizer.word_index) + 1
    num_classes = len(label_encoder.classes_)

    model = build_model(vocab_size, num_classes)
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    model.summary()

    ckpt = ModelCheckpoint(os.path.join(MODEL_DIR, "best_model.h5"),
                           save_best_only=True, monitor="val_loss")

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        class_weight=class_weights,
        callbacks=[ckpt, EarlyStopping(patience=6, restore_best_weights=True)],
        verbose=2
    )

    # Save tokenizer + encoder + final model
    pickle.dump(tokenizer, open(os.path.join(MODEL_DIR, "tokenizer.pkl"), "wb"))
    pickle.dump(label_encoder, open(os.path.join(MODEL_DIR, "label_encoder.pkl"), "wb"))
    model.save(os.path.join(MODEL_DIR, "final_model.keras"))

    # Evaluate
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print("\nTEST ACCURACY:", test_acc)

    # Confusion Matrix + F1
    y_pred = np.argmax(model.predict(X_test), axis=1)

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

    cm = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix:")
    print(cm)

    return model, tokenizer, label_encoder

# ----------------------
# Run
# ----------------------

if __name__ == "__main__":
    train_pipeline()